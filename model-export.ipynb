{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from onnxruntime_extensions import gen_processing_models\n",
    "from onnxruntime_extensions import get_library_path\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Step 1: Load the Huggingface Roberta tokenizer and model\n",
    "input_text = \"A test text!\"\n",
    "model_type = \"roberta-base\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_type, num_labels=3)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"best_roberta_model_ultimate.pt\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Model Inputs: ['input_text']\n",
      "Tokenizer Model Outputs: ['input_ids', 'attention_mask', 'offset_mapping']\n",
      "Model Inputs: ['input_ids', 'attention_mask_input']\n",
      "Model Outputs: ['logits']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-03-27 14:26:47.067884944 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running Expand node. Name:'/roberta/Expand_1' Status Message: invalid expand shape\u001b[m\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Expand node. Name:'/roberta/Expand_1' Status Message: invalid expand shape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     79\u001b[39m input_feed = {\n\u001b[32m     80\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_text\u001b[39m\u001b[33m\"\u001b[39m: np.asarray([input_text])\n\u001b[32m     81\u001b[39m }  \u001b[38;5;66;03m# Assuming \"input_text\" is the input expected by the tokenizer\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m outputs = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Print the outputs\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits:\u001b[39m\u001b[33m\"\u001b[39m, outputs[\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MalClass/.venv/lib/python3.13/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:270\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    268\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mInvalidArgument\u001b[39m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Expand node. Name:'/roberta/Expand_1' Status Message: invalid expand shape"
     ]
    }
   ],
   "source": [
    "# Step 2: Export the tokenizer to ONNX using gen_processing_models\n",
    "onnx_tokenizer_path = \"tokenizer.onnx\"\n",
    "\n",
    "# Generate the tokenizer ONNX model\n",
    "tokenizer_onnx_model, _ = gen_processing_models(tokenizer, pre_kwargs={})\n",
    "assert tokenizer_onnx_model is not None\n",
    "\n",
    "# Save the tokenizer ONNX model\n",
    "with open(onnx_tokenizer_path, \"wb\") as f:\n",
    "    f.write(tokenizer_onnx_model.SerializeToString())\n",
    "\n",
    "# Step 3: Export the Huggingface Roberta model to ONNX\n",
    "onnx_model_path = \"malware_classifier.onnx\"\n",
    "dummy_input = tokenizer(\n",
    "    \"This is a dummy input\",\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,  # model to be exported\n",
    "    (\n",
    "        dummy_input[\"input_ids\"],\n",
    "        dummy_input[\"attention_mask\"],\n",
    "    ),  # model input (dummy input)\n",
    "    onnx_model_path,  # where to save the ONNX model\n",
    "    input_names=[\"input_ids\", \"attention_mask_input\"],  # input tensor name\n",
    "    output_names=[\"logits\"],  # output tensor names\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  # dynamic axes\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Step 4: Merge the tokenizer and model ONNX files into one\n",
    "onnx_combined_model_path = \"combined_malware_classifier.onnx\"\n",
    "\n",
    "# Load the tokenizer and model ONNX files\n",
    "tokenizer_onnx_model = onnx.load(onnx_tokenizer_path)\n",
    "model_onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Inspect the ONNX models to find the correct input/output names\n",
    "print(\n",
    "    \"Tokenizer Model Inputs:\", [node.name for node in tokenizer_onnx_model.graph.input]\n",
    ")\n",
    "print(\n",
    "    \"Tokenizer Model Outputs:\",\n",
    "    [node.name for node in tokenizer_onnx_model.graph.output],\n",
    ")\n",
    "print(\"Model Inputs:\", [node.name for node in model_onnx_model.graph.input])\n",
    "print(\"Model Outputs:\", [node.name for node in model_onnx_model.graph.output])\n",
    "\n",
    "# Merge the tokenizer and model ONNX files\n",
    "combined_model = onnx.compose.merge_models(\n",
    "    tokenizer_onnx_model,\n",
    "    model_onnx_model,\n",
    "    io_map=[(\"input_ids\", \"input_ids\"), (\"attention_mask\", \"attention_mask_input\")],\n",
    ")\n",
    "# Save the combined model\n",
    "onnx.save(combined_model, onnx_combined_model_path)\n",
    "\n",
    "# Step 5: Test the combined ONNX model using an Inference session with ONNX Runtime Extensions\n",
    "# Initialize ONNX Runtime SessionOptions and load custom ops library\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.register_custom_ops_library(get_library_path())\n",
    "\n",
    "# Initialize ONNX Runtime Inference session with Extensions\n",
    "session = ort.InferenceSession(\n",
    "    onnx_combined_model_path,\n",
    "    sess_options=sess_options,\n",
    "    providers=[\"CPUExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# Prepare dummy input text\n",
    "input_feed = {\n",
    "    \"input_text\": np.asarray([input_text])\n",
    "}  # Assuming \"input_text\" is the input expected by the tokenizer\n",
    "\n",
    "# Run the model\n",
    "outputs = session.run(None, input_feed)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"logits:\", outputs[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
